# Downscaled climate data from PCIC {#pcic}

_Last update: `r file.info("10-PCICdata.Rmd")$mtime`_

```{r setup, include=FALSE}
library(reticulate)
conda <- ifelse(file.exists("/opt/conda/bin/conda"), "/opt/conda/bin/conda", "/data/home-ext/miniconda3/bin/conda")
use_condaenv("man_ccia", conda = conda, required = TRUE)
```

We can improve upon our estimate of the impact of climate change on our exposure unit of interest by downscaling GCM data using more sophisticated statistical downscaling techniques.

In this exercise, we will use data that has been downscaled using a bias-corrected constructed analogs method (BCCAQ; Werner & Cannon 2015). One of the advantages of this method is that we can bias-correct and downscale daily GCM data.

The data was provided by @pcic2014statistically, and is downscaled based on a high-quality gridded observed dataset (~10 km grid size) developed by NRCan (ANUSPLIN300). PCIC provides an excellent overview of their downscaling methodology, with citations made to the relevant articles, [here](https://
www.pacificclimate.org/data/statistically-downscaled-climate-scenarios). You should read the entirety of that page before continuing this preface and before starting lab 5.

## Retreiving downscaled data from the data portal website

PCIC provides an intutive point and click map for accessing the downscaled GCM data. In our exercise in Lab 5, we will use six models (CanESM2, CCSM4, CSIRO-Mk3-6-0, GFDLESM2G, inmcm4 and MIROC5). To download this information manually, follow the steps below. You may instead wish to download these data programatically. Skip to the next section for that process. 

1. Open the link to the downscaled GCM data: http://tools.pacificclimate.org/dataportal/downscaled_gcms/map/. A map of North America should appear on your screen.
2. In the bar on the right-hand side of your screen. Select "historical + rcp45".
3. For this exercise, we will only use the BCCAQ downscaled data.
4. Select one of the downscaled datasets, e.g. BCCAQ+ANUSPLIN300+inmcm4 and click on 'tasmax'. We have to download 'tasmax' and 'tasmin' separately.
5. Set your start date to 1981-01-01, leave the end date as is at 2100-12-31
5. Now, click on the pen icon in the top right-hand corner of the map and select the Toronto latitude and longitude (43.67°N and 79.40°W). A small window will pop-up asking if you want to download this data - click 'OK'.
6. Repeat this process 23 times so that you have data for both 'tasmax' and 'tasmin' for all 6 models for RCP 4.5 and RCP 8.5. 

## Retreiving downscaled GCM data from PCIC programatically

The The Pacific Climate Impacts Consortium (PCIC) data portal can also be accessed programatically via `wget` or your preferred programming language. If you performed any manual downloading, above, you may have noticed that the data portal webpage returns some interesting data set URLs. We can use the same specific URL syntax to automate our downloading. We can do this in Python using the code, below. 

As always, we first import the necessary libraries.

```{python}
import urllib.request
import json
from netCDF4 import Dataset, date2index, num2date, date2num
import datetime as dt
import numpy as np
import re
```

PCIC maintains a JSON-formatted catalogue of file IDs and URLs, which we can retreive in Python directly. 

```{python}
with urllib.request.urlopen("http://tools.pacificclimate.org/dataportal/downscaled_gcms/catalog/catalog.json") as url:
    data = json.loads(url.read().decode())
```

We cannot access the data entries with an integer index, but rather need to access the data using the keys. We can print the fill list of keys in the data object using `data.keys()`. This is a very long list, so I will omit it here. Let's say that we are interested in accessing the dataset with the key `pr-tasmax-tasmin_day_BCSD-ANUSPLIN300-CanESM2_historical-rcp85_r1i1p1_19500101-21001231`. We can find the URL for this like this: 

```{python}
url = data['pr-tasmax-tasmin_day_BCSD-ANUSPLIN300-CanESM2_historical-rcp85_r1i1p1_19500101-21001231']
print(url)
```

Since this is a netCDF file, we can access its dimensions directly, just like we have done for files stored on the disk.

```{python}
nc = Dataset(url)
nc_time = nc.variables['time']
nc_lat = nc.variables['lat'][:].data
nc_lon = nc.variables['lon'][:].data
```

Now we can find the relevant indeces.

```{python}
target = {"lat": 43.67, "lon": -79.40}
total_time = len(nc_time)
time_start = date2index(dt.datetime(1981, 1, 1), nc_time, select="nearest")
lat_cell = np.argmin(np.abs(nc_lat - target['lat']))
lon_cell = np.argmin(np.abs(nc_lon - target['lon']))
print("Our start date is", num2date(nc_time[time_start], nc_time.units, nc_time.calendar))
print("We need data starting at time T={} (of {}), from X={} and Y={}".format(time_start, total_time, lon_cell, lat_cell))
```

Now that we know the specific dimensions that we need to download, we can find the keys for the full set of models that we desire, for both RCP4.5 and RCP8.5. We will use a special search pattern called a [regular expression](https://en.wikipedia.org/wiki/Regular_expression) for this. 

```{python}
models = ["CanESM2", "CCSM4", "CSIRO-Mk3-6-0", "GFDL-ESM2G", "inmcm4", "MIROC5"]
```

Here we are collapsing our list of models using '`|`', which operates in our regular expression to mean "OR". The same goes for our scenarios "`rcp(45|85)`" will match both "rcp45" and "rcp85".


```{python}
pattern = r'.*BCCAQ.*(' + '|'.join(models) + r').*-rcp(45|85).*'
print(pattern)
```

Now we can pull out our desired keys

```{python}
keys = [i for i in data.keys() if re.match(pattern, i)]
print(keys)
```

You might expect that you can access the data in the PCIC netCDF files directly, just like we have done for the dimensions. This is usually true of netCDF files but at the time of writing, trying to access the files directly was [giving unexpected results](https://github.com/pacificclimate/pdp/issues/94). However, we can download the required files to disk and access them locally. We will use the `urlretrieve` method of the **urllib.request** module for this.

Before we download the files, I want to generate some filenames, replacing the original "pr-tasmax-tasmin" prefix with the specific variable ("tasmax" or "tasmin") that I want to store in each file (note that it is possible to download two variables at once, but here I will keep it simple and download each one at a time. I will also correct the start date for good measure.


```{python}
filenames = list(map(lambda i: re.sub("pr-tasmax-tasmin", "tasmax", i) + ".nc", keys)) + \
              list(map(lambda i: re.sub("pr-tasmax-tasmin", "tasmin", i) + ".nc", keys))
filenames = [re.sub('19500101', '19810101', i) for i in filenames]
```

The data portal [documentation](https://data.pacificclimate.org/portal/docs/raster.html#power-user-howto) explains that we need to format our request URL with a valid [OPeNDAP](http://opendap.org/) specification. In our case, the URL will look like this:

`http://data.pacificclimate.org/dataportal/downscaled_gcms/data/<dataset_id>.nc?<variable>[<time range>][<lon range>][<lat range>]`

We need to fill in the `dataset_id` with the URL from the catalogue that we downloaded earlier. We will complete the DAP specification with either "tasmax" or "tasmin" and `[time_start:time_total][lon_cell:lon_cell][lat_cell:lat_cell]`. You may notice that the requested URL has the `.nc` extension twice. This is because we are asking the server to access the netCDF file and to return the result as a netCDF file. Other options include `.csv` and ASCII gridded data, but these are metadata-poor formats and are not recommended.

```{python}
url_suffix = "[{}:{}][{}:{}][{}:{}]".format(time_start, total_time, lat_cell, lat_cell, lon_cell, lon_cell)
urls = list(map(lambda i: data[i] + ".nc?tasmax" + url_suffix, keys)) + \
         list(map(lambda i: data[i] + ".nc?tasmin" + url_suffix, keys))
```

You may wish to confirm that your filenames and URLs match, e.g.

```{python}
print(filenames[0])
```

```{python}
print(urls[0])
```

Now you can proceed to download these files by looping over the `filenames` and `urls` objects. Note that, at the time of writing, the download speeds were _very_ slow (< 500 bits/second or about 0.0625 kb/s). As such, we have mirrored the data necessary for this assignment at [insert link here](example.com).

```{python, eval=FALSE}
for i in range(len(urls)):
    urllib.request.urlretrieve(urls[i], filenames[i])
```



Now that you have the data