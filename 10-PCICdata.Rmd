# Downscaled climate data from PCIC {#pcic}

_Last update: `r system("git log -1 --format=\"%ad (%h)\" -- 10-PCICdata.Rmd", intern = TRUE)`_

```{r setup, include=FALSE}
library(reticulate)
conda <- ifelse(file.exists("/opt/conda/bin/conda"), "/opt/conda/bin/conda", "/data/home-ext/miniconda3/bin/conda")
use_condaenv("man_ccia", conda = conda, required = TRUE)
source("R/deco_hook.R")
```

```{python py_setup, include=FALSE}
import matplotlib as mpl
mpl.use("agg") # Needed for the book only
mpl.rc('font', size=12)
from matplotlib import pyplot as plt
plt.switch_backend('agg') # Might be overkill
import numpy as np
import pandas as pd
import scipy.stats as stats
import os
import cftime
from netCDF4 import Dataset, num2date
```

We can improve upon our estimate of the impact of climate change on our exposure unit of interest by downscaling GCM data using more sophisticated statistical downscaling techniques.

In this exercise, we will use data that has been downscaled using a bias-corrected constructed analogs method (BCCAQ; see @werner2016hydrologic). One of the advantages of this method is that we can bias-correct and downscale daily GCM data.

The data was provided by @pcic2014statistically, and is downscaled based on a high-quality gridded observed dataset (~10 km grid size) developed by NRCan (ANUSPLIN300). PCIC provides an excellent overview of their downscaling methodology, with citations made to the relevant articles, [here](https://
www.pacificclimate.org/data/statistically-downscaled-climate-scenarios). You should read the entirety of that page before continuing this preface and before starting lab 5.

Figure \@ref(fig:l5f0), below, shows the results of this downscaling process for the Canadian CanESM2 model. Notice that the PCIC product, downscaled to a 10 km grid, is much more closely related to the observed station point data that we downloaded from Environment and Climate Change Canada. Our comparison here still isn't perfect, as we're still comparing a point to a grid cell, but as our grid cell decreases in size, the two series begin to behave more similarly. This definitely looks like a solution to improve the quality of our impact assessment. Read on to learn how to obtain this high-resolution downscaled data directly from PCIC.

```{python make_ds_comp_plot, include=FALSE}
## Downscaled Data

filename = {'tasmax': 'data/pcic_data/tasmax_day_BCCAQ-ANUSPLIN300-CanESM2_historical-rcp45_r1i1p1_19810101-21001231.nc',
            'tasmin': 'data/pcic_data/tasmin_day_BCCAQ-ANUSPLIN300-CanESM2_historical-rcp45_r1i1p1_19810101-21001231.nc'}

nc = Dataset(filename['tasmax'])
nc_time = nc.variables['time']
time = num2date(nc_time[:], nc_time.units, nc_time.calendar)
nc_var = np.squeeze(nc.variables['tasmax'][:].data)
nc.close()
df = pd.DataFrame({'Date': time, 'Model': 'CanESM2', 'Variable': 'tasmax', 'Value': nc_var})

nc = Dataset(filename['tasmin'])
nc_var = np.squeeze(nc.variables['tasmin'][:].data)
nc.close()
df = df.append(pd.DataFrame({'Date': time, 'Model': 'CanESM2', 'Variable': 'tasmin', 'Value': nc_var}))   

df = df.pivot_table(
    index=['Date', 'Model'],
    columns='Variable',
    values='Value').reset_index()

df['tas'] = (df.tasmax + df.tasmin)/2
df = df.melt(id_vars=['Date','Model'], value_name="Value")
baseline = df[(df.Date >= cftime.DatetimeNoLeap(1981, 1, 1)) & (df.Date <= cftime.DatetimeNoLeap(2010, 12, 31))]

## Observed Data

T_STN = pd.read_csv("tor.csv", index_col = 0)

## Raw CanESM2 Data

T_hist_CanESM2 = np.genfromtxt('data/CanESM2_tas_19812010_mat.csv', delimiter=',').ravel()

## Plot PDFs of all data for Baseline time period

kernel1 = stats.gaussian_kde(np.asarray(baseline.Value[baseline.Variable=='tas']))
x1 = np.linspace(2*np.asarray(baseline.Value[baseline.Variable=='tas']).min(), 
                 2*np.asarray(baseline.Value[baseline.Variable=='tas']).max(), 1000)
pdf1 = kernel1(x1)

kernel2 = stats.gaussian_kde(np.asarray(T_STN.MeanTemp)[~np.isnan(T_STN.MeanTemp)])
x2 = np.linspace(2*np.asarray(T_STN.MeanTemp)[~np.isnan(T_STN.MeanTemp)].min(), 2*np.asarray(T_STN.MeanTemp)[~np.isnan(T_STN.MeanTemp)].max(), 1000)
pdf2 = kernel2(x2)

kernel3 = stats.gaussian_kde(T_hist_CanESM2)
x3 = np.linspace(2*T_hist_CanESM2.min(), 2*T_hist_CanESM2.max(), 1000)
pdf3 = kernel3(x3)

fig = plt.figure(figsize=(9,6))

plt.plot(x1, pdf1, color='blue',linewidth=2)
plt.plot(x2, pdf2, color='green',linewidth=2)
plt.plot(x3, pdf3, color='orange',linewidth=2)

plt.plot(np.squeeze(np.mean(np.asarray(T_STN.MeanTemp)[~np.isnan(T_STN.MeanTemp)])*np.ones(x1.size)),
         np.linspace(0,0.045,x1.size),'green',linewidth=2,label='Station (point data)')
plt.plot(np.squeeze(np.mean(np.asarray(baseline.Value[baseline.Variable=='tas']))*np.ones(x1.size)),
         np.linspace(0,0.045,x1.size),'blue',linewidth=2,label='DS CanESM2 (~10 km grid)')
plt.plot(np.squeeze(np.mean(T_hist_CanESM2)*np.ones(x1.size)),
         np.linspace(0,0.045,x1.size),'orange',linewidth=2,label='Raw CanESM2 (~2.8125° grid)')

plt.xlabel('$T_{mean}$ ($^{\circ}$C)')
plt.ylabel('Probability Density')
plt.title('$T_{mean}$ at Toronto, 1981‒2010')
plt.legend(loc='upper right',fontsize=12)
plt.ylim(0,0.045)
plt.tight_layout()
plt.savefig('l5f0.png')
plt.clf()
```

```{r l5f0, echo=FALSE, fig.cap="Baseline (1981\u20122100) $T_{mean}$ at Toronto, observed (green), simulated by CanESM2 (yellow), and downscaled by PCIC (blue)."}
knitr::include_graphics("l5f0.png", dpi = NA)
```

## Retrieving downscaled data from the data portal website

PCIC provides an intuitive point and click map for accessing the downscaled GCM data. In our exercise in Lab 5, we will use six models (CanESM2, CCSM4, CSIRO-Mk3-6-0, GFDLESM2G, inmcm4 and MIROC5). To download this information manually, follow the steps below. You may instead wish to download these data programatically. Skip to the next section for that process. 

1. Open the link to the downscaled GCM data: http://tools.pacificclimate.org/dataportal/downscaled_gcms/map/. A map of North America should appear on your screen.
2. In the bar on the right-hand side of your screen. Select "historical + rcp45".
3. For this exercise, we will only use the BCCAQ downscaled data.
4. Select one of the downscaled datasets, e.g. BCCAQ+ANUSPLIN300+inmcm4 and click on 'tasmax'. We have to download 'tasmax' and 'tasmin' separately.
5. Set your start date to 1981-01-01, leave the end date as is at 2100-12-31
5. Now, click on the pen icon in the top right-hand corner of the map and select the Toronto latitude and longitude (43.67°N and 79.40°W). A small window will pop-up asking if you want to download this data - click 'OK'.
6. Repeat this process 23 times so that you have data for both 'tasmax' and 'tasmin' for all 6 models for RCP 4.5 and RCP 8.5. 

## Retrieving downscaled GCM data from PCIC programatically

The The Pacific Climate Impacts Consortium (PCIC) data portal can also be accessed programatically via `wget` or your preferred programming language. If you performed any manual downloading, above, you may have noticed that the data portal webpage returns some interesting data set URLs. We can use the same specific URL syntax to automate our downloading. We can do this in Python using the code, below. 

As always, we first import the necessary libraries.

```{python, deco=list()}
import urllib.request
import json
from netCDF4 import Dataset, date2index, num2date, date2num
import datetime as dt
import numpy as np
import re
```

PCIC maintains a JSON-formatted catalogue of file IDs and URLs, which we can retrieve in Python directly. 

```{python, deco=list()}
with urllib.request.urlopen("http://tools.pacificclimate.org/dataportal/downscaled_gcms/catalog/catalog.json") as url:
    data = json.loads(url.read().decode())
```

We cannot access the data entries with an integer index, but rather need to access the data using the keys. We can print the fill list of keys in the data object using `data.keys()`. This is a very long list, so I will omit it here. Let's say that we are interested in accessing the dataset with the key `pr-tasmax-tasmin_day_BCSD-ANUSPLIN300-CanESM2_historical-rcp85_r1i1p1_19500101-21001231`. We can find the URL for this like this: 

```{python, deco=list()}
url = data['pr-tasmax-tasmin_day_BCSD-ANUSPLIN300-CanESM2_historical-rcp85_r1i1p1_19500101-21001231']
print(url)
```

Since this is a netCDF file, we can access its dimensions directly, just like we have done for files stored on the disk.

```{python, deco=list()}
nc = Dataset(url)
nc_time = nc.variables['time']
nc_lat = nc.variables['lat'][:].data
nc_lon = nc.variables['lon'][:].data
```

Now we can find the relevant indices.

```{python, deco=list()}
target = {"lat": 43.67, "lon": -79.40}
total_time = len(nc_time)
time_start = date2index(dt.datetime(1981, 1, 1), nc_time, select="nearest")
lat_cell = np.argmin(np.abs(nc_lat - target['lat']))
lon_cell = np.argmin(np.abs(nc_lon - target['lon']))
print("Our start date is", num2date(nc_time[time_start], nc_time.units, nc_time.calendar))
print("We need data starting at time T={} (of {}), from X={} and Y={}".format(time_start, total_time, lon_cell, lat_cell))
```

Now that we know the specific dimensions that we need to download, we can find the keys for the full set of models that we desire, for both RCP4.5 and RCP8.5. We will use a special search pattern called a [regular expression](https://en.wikipedia.org/wiki/Regular_expression) for this. 

```{python, deco=list()}
models = ["CanESM2", "CCSM4", "CSIRO-Mk3-6-0", "GFDL-ESM2G", "inmcm4", "MIROC5"]
```

Here we are collapsing our list of models using '`|`', which operates in our regular expression to mean "OR". The same goes for our scenarios "`rcp(45|85)`" will match both "rcp45" and "rcp85".


```{python, deco=list()}
pattern = r'.*BCCAQ.*(' + '|'.join(models) + r').*-rcp(45|85).*'
print(pattern)
```

Now we can pull out our desired keys

```{python, deco=list()}
keys = [i for i in data.keys() if re.match(pattern, i)]
print(keys)
```

You might expect that you can access the data in the PCIC netCDF files directly, just like we have done for the dimensions. This is usually true of netCDF files but at the time of writing, trying to access the files directly was [giving unexpected results](https://github.com/pacificclimate/pdp/issues/94). However, we can download the required files to disk and access them locally. We will use the `urlretrieve` method of the **urllib.request** module for this.

Before we download the files, I want to generate some filenames, replacing the original "pr-tasmax-tasmin" prefix with the specific variable ("tasmax" or "tasmin") that I want to store in each file (note that it is possible to download two variables at once, but here I will keep it simple and download each one at a time. I will also correct the start date for good measure.


```{python, deco=list()}
filenames = list(map(lambda i: re.sub("pr-tasmax-tasmin", "tasmax", i) + ".nc", keys)) + \
              list(map(lambda i: re.sub("pr-tasmax-tasmin", "tasmin", i) + ".nc", keys))
filenames = [re.sub('19500101', '19810101', i) for i in filenames]
```

The data portal [documentation](https://data.pacificclimate.org/portal/docs/raster.html#power-user-howto) explains that we need to format our request URL with a valid [OPeNDAP](http://opendap.org/) specification. In our case, the URL will look like this:

`http://data.pacificclimate.org/dataportal/downscaled_gcms/data/<dataset_id>.nc?<variable>[<time range>][<lon range>][<lat range>]`

We need to fill in the `dataset_id` with the URL from the catalogue that we downloaded earlier. We will complete the DAP specification with either "tasmax" or "tasmin" and `[time_start:time_total][lon_cell:lon_cell][lat_cell:lat_cell]`. You may notice that the requested URL has the _.nc_ extension twice. This is because we are asking the server to access the netCDF file and to return the result as a netCDF file. Other options include _.csv_ and ASCII gridded data, but these are metadata-poor formats and are not recommended.

```{python, deco=list()}
url_suffix = "[{}:{}][{}:{}][{}:{}]".format(time_start, total_time, lat_cell, lat_cell, lon_cell, lon_cell)
urls = list(map(lambda i: data[i] + ".nc?tasmax" + url_suffix, keys)) + \
         list(map(lambda i: data[i] + ".nc?tasmin" + url_suffix, keys))
```

You may wish to confirm that your filenames and URLs match, e.g.

```{python, deco=list()}
print(filenames[0])
```

```{python, deco=list()}
print(urls[0])
```

Now you can proceed to download these files by looping over the `filenames` and `urls` objects. Note that, at the time of writing, the download speeds were _very_ slow (< 500 bits/second or about 0.0625 kb/s). In addition, it seems that subsetting in the time dimension causes downloads to fail after about 3kb are downloaded, so you may in fact need to download the entire time series. Given these current issues, you may prefer to download the data from [this book's repository](https://gitlab.com/claut/man_ccia/tree/master/data/pcic_data).

```{python, eval=FALSE, deco=list()}
for i in range(len(urls)):
    urllib.request.urlretrieve(urls[i], filenames[i])
```

Now that you have the data, we can proceed to analyze it. In Lab 5 (Sec. \@ref{lab5}), we will compare the downscaled GCM data to our observed data and re-assess our exposure unit based on these new data.
