# Lab exercise 1: Understanding the past {#lab1}

```{r setup, include=FALSE}
library(reticulate)
conda <- ifelse(file.exists("/opt/conda/bin/conda"), "/opt/conda/bin/conda", "/data/home-ext/miniconda3/bin/conda")
use_condaenv("book", conda = conda, required = TRUE)
```

```{python py_setup, include=FALSE}
import matplotlib as mpl
mpl.use("agg") # Needed for the book only
mpl.rc('font', size = 12)
from matplotlib import pyplot as plt
plt.switch_backend('agg') # Might be overkill
import numpy as np
import pandas as pd
pd.set_option('display.max_columns', 10)
import scipy.stats as stats
```


```{bash get_csvs, include=FALSE}
wget https://gitlab.com/ConorIA/shell-scripts/raw/master/eccc/eccc
chmod a+x eccc
./eccc -y 1981:2010 -s 5051 -x 24 -g
./eccc -y 2000:2010 -s 31688 -x 24 -g
```

One of the first steps of a climate change impact assessment (CCIA) is to establish a climatological baseline  for the climate variable of interest (either an exposure unit or the climate variable most directly linked to the exposure unit). 

An __exposure unit__ is an element or variable that is directly or indirectly affected by the changes in climate. In this exercise you will acquire Canadian climate data for Toronto and perform an analysis of an exposure unit that will be assigned to you. Refer to the Appendix (\@ref(appendix)) for a list of some example exposure units. 

What do we mean by baseline? A baseline period is needed to define the observed climate. This observed climate information is combined with GCM output of climate change to determine the climate change impact for a given future climate scenario (e.g. Representative Concentration Pathway (RCP)). When using climate model output, the baseline also serves as the reference period from which the modelled future change in climate is calculated. Future climate change is defined by comparing the years in the baseline period with the similar number of years in the future for a given future climate scenario.

Environment and Climate Change Canada (ECCC) makes data available over an internet portal. You can [visit this page](https://climate.weather.gc.ca/), and donwload data manually, but that is a _very_ tedious process. In this lab, I will present you with two options for downloading Canadian climate data: the command-line, and via an R package. 

## Getting the data

### Option 1: On the command line
<details>
  <summary>Click here for instructions.</summary>
ECCC makes data available over an [FTP site](ftp://client_climate@ftp.tor.ec.gc.ca/Pub/Get_More_Data_Plus_de_donnees/), and gives instructions for downloading data manually via `bash` and `wget`. You can find instructions for using the command line to download data [here](ftp://client_climate@ftp.tor.ec.gc.ca/Pub/Get_More_Data_Plus_de_donnees/Readme.txt).

#### A scripted solution

For convenience, I have written a script, called `eccc` to perform this downloading. You can get the latest version of this script from [GitLab](https://gitlab.com/ConorIA/shell-scripts/raw/master/eccc/eccc).

To get help using the script, pass the `-h` flag to the script. 

```{bash eccc_help}
./eccc -h
```

This command prints out help information for using the script. To download daily data for Toronto, for instance, you can run the script like this: 

```{bash, eval=FALSE}
./eccc -y 1981:2010 -s 5051 -x 24 -g
```

This will download the individual year-by-year `.csv` files for daily data at Toronto, crop the first 24 lines from them, and the glue them all together in a single file. In this case. That file is called "`r list.files(pattern = "5051.*csv")`".

#### Cleaning the data in Python

Now that we have the data that we need, we need to start working with it. Fire up your installation of Python, and follow along with the following commands. 

First, let's import the libraries we are going to need to manipulate our data. 

```{python, eval = FALSE}
import numpy as np
import pandas as pd
```

Now read the `.csv` files into Python. 

```{python}
tor = pd.read_csv("5051-daily.csv")
```

Let's take a look at what our columns are named. 

```{python}
print(tor.columns)
```

Hmm, some of those are very clunk. Let's rename the colums we are going to use and then select only those columns. 

```{python}
tor = tor.rename(columns={'Date/Time': 'Date', 'Max Temp (°C)': 'MaxTemp', 'Min Temp (°C)': 'MinTemp', 'Mean Temp (°C)': 'MeanTemp', 'Total Precip (mm)': 'TotalPrecip'})
tor = tor[['Date', 'MaxTemp', 'MinTemp', 'MeanTemp', 'TotalPrecip']]
```

Great. Let's take a look at what we've got. 

```{python}
print(tor.head())
```
```{python}
print(tor.tail())
```

Uhh ohh! It looks like we are missing all of our temperature data at the end of our `.csv` file. We can test this theory by using an enumerator. The following will tell us the index at which all three of our temperature variables of interest are missing for the first time. 
```{python}
print(next(i for i, x in enumerate(tor.loc[:, 'MaxTemp':'MeanTemp'].isnull().all(axis=1)) if x) - 1)
```

Let's take a look at the context around that index. 

```{python}
print(tor[8211:8221])
```

It looks like we're missing data as of July 2003. Indeed, in 2003 the "Toronto" station was renamed "Toronto City" and re-coded. Since that date, the daily data is available under station code 31688. 

We'll have to return to `bash` for a moment to get the data that we're missing. Let's grab the data for the new station code from 2000 to 2010. 

<center>**Really, run this in bash, not Python**<\center>
```{bash, eval = FALSE}
./eccc -y 2000:2010 -s 31688 -x 24 -g
```

Now let's return to Python. 

<center>**Back to Python**<\center>
```{python}
tor2 = pd.read_csv("31688-daily.csv")
tor2 = tor2.rename(columns={'Date/Time': 'Date', 'Max Temp (°C)': 'MaxTemp', 'Min Temp (°C)': 'MinTemp', 'Mean Temp (°C)': 'MeanTemp', 'Total Precip (mm)': 'TotalPrecip'})
tor2 = tor2[['Date', 'MaxTemp', 'MinTemp', 'MeanTemp', 'TotalPrecip']]
```

We can check the head and the tail of this data too. 

```{python}
print(tor2.head())
```
```{python}
print(tor2.tail())
```

As expected, we're missing data at the beginning of the file (before 2003). We can use a slightly modified version of our iterator from earlier. This time, we'll check for the first row for which any of the data is present. 

```{python}
print(next(i for i, x in enumerate(tor2.loc[:, 'MaxTemp':'MeanTemp'].notnull().any(axis=1)) if x) - 1)
```

We'll take a look at that in context again. 

```{python}
print(tor2[880:890])
```

It looks like our data starts as of June 04, 2002. 

Ok, so we know that we have data at the old station until May 30, 2002. Likewise, we have some data at the new station in May 2002. Let's take a look at the overlap and see if we can merge these files without issue. 

```{python}
print(tor[(tor.Date >= "2003-06-25") & (tor.Date <= "2003-07-05")])
```
```{python}
print(tor2[(tor2.Date >= "2003-06-25") & (tor2.Date <= "2003-07-05")])
```

The data on the overlapping days is virtually identical between the two station codes, so we can be confident that merging these two data sets won't lead to a sudden temperature bump. Let's append the relevant section of `tor2` to `tor`.

```{python}
tor = tor[tor.Date < "2003-07-01"].append(tor2[tor2.Date >= "2003-07-01"])
```

Now we can make sure that we have a full data set from January 1981 to December 2010. 

```{python}
print(tor.head())
```
```{python}
print(tor.tail())
```

```{bash cleanup, include=FALSE}
rm eccc* *.csv
```
</details>

### Option 2: Using the `canadaHCD` package in R
<details>
<summary>Click here for instructions.</summary>
For those of us who work often in R, the `canadaHCD` package [@R-canadaHCD] is the first stop for Canadian climate data. We can take advantage of this package in Python too, thanks to the `rpy2` Python module. We will also use an expansion pack that I have written based on `canadaHCD`, called `canadaHCDx` [@R-canadaHCDx]. Of course, you are free to run any of the R commands in the code below directly in R.

[//]: # (It seems (understandably), that calling rpy2 through reticulate causes all sorts of trouble)

```{block, type='rmdcomment'}
In the interest of full disclosure, it should be noted that your output might look slightly different to what you see below. This book was written in R. As you might imagine, it is not possible to run R via Python via R! I have, therefore, provided the code below in the context of Python to R, but am using R "under the hood" to generate the data. 
```

First we need to import our Python libraries.

```{python, eval=FALSE}
import pandas as pd
from rpy2.robjects import r, pandas2ri
```

We now have an object in Python called `r`, that serves as an interface to R. You can pass code to R using `r('your code goes here')`. 

```{python, eval=FALSE}
print(r('R.version.string'))
```
```{r, include=FALSE}
rver <- R.version.string
```
```{python, echo=FALSE}
print(r.rver)
```

In R, we load packages using the `library()` function. You can load a package into your `rpy2` R environment like this:

```{python, eval = FALSE}
r('library(canadaHCDx)')
```

You may see some errors about masking. Both `canadaHCD` and `canadaHCDx` include a `find_station()` function to search for available station data. The more advanced 'x' version masks the former. 

```{python, eval=FALSE}
print(r('find_station("Toronto")'))
```
```{r, include=FALSE}
search <- canadaHCD::find_station("Toronto")
```
```{python, echo = FALSE}
print(r.search)
```

Depending on the platform that you are using, this table might be hard to read. Jupyter Notebook maintains the format as a `R/rpy2 DataFrame` object that abbreviates all the columns. You can convert it into a Python pandas using the `pandas2ri.ri2py()` method. 

```{python, eval=FALSE}
search = pandas2ri.ri2py(search)
```

You can also automatically convert all subsequent R objects to Python by running `pandas2ri.activate()`.

So, we have a _very_ long list of stations, but we have no idea about the kinds of data that are available. `canadaHCDx` includes some more advanced search options.

```{python, eval=FALSE}
r('find_station("Toronto", baseline = 1981:2010, type = "daily")')
```
```{r, include=FALSE}
search <- canadaHCDx::find_station("Toronto", baseline = 1981:2010, type = "daily", assume_yes = TRUE)
```
```{python, echo=FALSE}
print(r.search)
```

It looks like Toronto (5051) has all the data we need. Let's download it.

```{python, eval=FALSE}
print(r('tor <- hcd_daily(5051, 1981:2010, progress = FALSE)').head())
```
```{r, include=FALSE}
tor <- canadaHCD::hcd_daily(5051, 1981:2010, progress = FALSE)
```
```{python, echo=FALSE}
print(r.tor.head())
```

In the above cell, we saved the data into an object on the R "side". We can pull the object into Python via assignment. 

```{python, eval=FALSE}
tor = r('tor')
print(tor.head())

```
```{python, echo=FALSE}
tor = r.tor
print(tor.head())
```

You may encounter that some of the columns become corrupted. The most important among these is the data column. If you find that you have a series of numbers instead of dates, you can easily overwrite the column by:

```{python}
tor['Date'] = pd.date_range(start="1981-01-01", end="2010-12-31")
print(tor.head())
```

It looks like there is something wrong with the `MaxGustDir` and `MaxGustSpeed` columns, but we won't be using them for anything, so we can safely ignore them. Indeed, let's drop the columns that aren't of interest to us. 

```{python}
tor = tor[['Station', 'Date', 'MaxTemp', 'MinTemp', 'MeanTemp', 'TotalPrecip']]
```

Let's check the end of our data. 

```{python}
print(tor.tail())
```

Uhh ohh! It looks like there are a lot of missing temperature values at the end of the data set. Let's look for another station, within 1 km of Toronto that will give us more data. 

```{r, include=FALSE}
search <- canadaHCDx::find_station(target = 5051, dist = 0:1)
```
```{python, eval=FALSE}
r('find_station(target = 5051, dist = 0:1)')
```
```{python, echo=FALSE}
print(r.search)
```

It seems that there was a station code change at Toronto in 2003. The "Toronto" station, 5051, became "Toronto City", 31688. These are co-located. These data sets can be easily merged between 2002 and 2003. 


```{python, eval=FALSE}
t1 = r('hcd_daily(5051, 1981:2002, progress = FALSE)')
t2 = r('hcd_daily(31688, 2003:2010, progress = FALSE)')
```
```{r, include=FALSE}
t1 <- canadaHCD::hcd_daily(5051, 1981:2002, progress = FALSE)
t2 <- canadaHCD::hcd_daily(31688, 2003:2010, progress = FALSE)
```
```{python, include=FALSE}
t1 = r.t1
t2 = r.t2
```

Merge the tables using the pd.append() method.

```{python}
tor = t1.append(t2)
```

Now let's clean this up a little.

```{python}
tor['Date'] = pd.date_range(start="1981-01-01", end="2010-12-31")
tor = tor[['Station', 'Date', 'MaxTemp', 'MinTemp', 'MeanTemp', 'TotalPrecip']]
print(tor.head())
```

```{python}
print(tor.tail())
```

There you have it! We just downloaded 30 years of data from Environment Canada in Python via R!
</details>

## Analyzing the data

Now let's take a look at our data. We will require some additional libraries.

```{python, eval=FALSE}
import matplotlib as mpl
mpl.rc('font', size = 12)
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import scipy.stats as stats
```

As we mentioned in the introduction, an exposure unit is often the "unit" of our analysis when we want to perform an impact assessment. In this section, we'll calculate two exposure units, starting with a rather simple Boolean (True/False) unit. 

As I write this section, the temperature outside is a sweltering 32 °C (36 with the humidex), so what better exposure unit to consider than "tropical nights", nights with a minimum temperature above 20 °C. We will limit our analysis to the summer months.

This code requires that your "Date" column is recognized as such by Python. If you get an error, try the following expression in Python, `tor.Date = pd.to_datetime(tor.Date)`.

Let's start by adding a new column to our table called "TropNight", which will be a True/False value.

```{python}
tor['TropNight'] = ((tor.MinTemp > 20) & (tor.Date.dt.month.isin([6, 7, 8])))
print(tor.head())
```

Since our value is Boolean, we can use that column as an index. Let's double-check to make sure that we got the results that we were expecting.

```{python}
print(tor[tor.TropNight].head())
```

Now let's get an aggregate data set by grouping by the year. 

```{python}
tor_trop = tor[['Date', 'TropNight']].groupby(tor.Date.dt.year).sum()
print(tor_trop.head())
```

We can plot this easily. Note, this book is generated on a machine with no display, if you want to see the plot, run `plot.show()` or set the option `%matplotlib inline` if you are using Jupyter Notebook.

```{python}
tor_trop.plot()
```
```{python, include=FALSE}
plt.savefig('l1f1.png')
plt.clf()
```

![](l1f1.png)

The grouping operation has changed our table index to the years from 1981 to 2010. We want to keep this information in the table, so we can reset our index like this:

```{python}
tor_trop.reset_index(level = 0, inplace = True)
print(tor_trop.head())
```

Now let's make another plot, this time with a trendline. 

```{python}
plt.plot(tor_trop.Date, tor_trop.TropNight)
plt.plot(tor_trop.Date, np.polyval(np.polyfit(tor_trop.Date, tor_trop.TropNight, 1), tor_trop.Date))
plt.title("Tropical Nights (JJA) at Toronto (1981‒2010)")
plt.xlabel("Year")
```
```{python, include = FALSE}
plt.savefig('l1f2.png')
plt.clf()
```

![](l1f2.png)

Let's try for a slightly more complicated exposure unit: Winter heating degree days (HDDs). The HDDs represent the difference between the mean temperature and the base temperature of 18 °C. Since HDDs can't be negative, they are only calculated when the mean temperature is below 18 °C. They are used as an indirect measure of energy demand. 

We can apply an anonymous (lambda) function to our `MinTemp` column to capture the following pseudocode expression:

<center>`return 0 if the mean temperature was above 18, otherwise, return the difference between 18 and the mean temperature`</center>

```{python}
tor['HDD'] = tor.MinTemp.apply(lambda x: 0 if x >= 18 else 18 - x)
```

We have one more challenge to overcome. A common mistake when performing seasonal analysis on the winter is to simply group December, January, and February by year. This is an easy logical jump to make. Remember, when we refer to winter, we mean the _continuous_ winter season that stretches from the December of the previous year to February of the current year. The easiest way to control for this is to add 1 to the year for any December.

```{python}
tor['Year'] = (lambda x: tor.Date.dt.year if x.equals(12) else tor.Date.dt.year + 1)(tor.Date.dt.month)
```

Now we can aggregate our heating degree days. Pay close attention to the code below. Here we are selecting two columns, filtering to just our winter months, and aggregating by `sum()`. 

```{python}
tor_hdd = tor[['Year', 'HDD']][tor.Date.dt.month.isin([12, 1, 2])].groupby('Year').sum()
tor_hdd.reset_index(level = 0, inplace = True)
print(tor_hdd.head())
```

```{python}
plt.plot(tor_hdd.Year, tor_hdd.HDD)
plt.plot(tor_hdd.Year, np.polyval(np.polyfit(tor_hdd.Year, tor_hdd.HDD, 1), tor_hdd.Year))
plt.title("Winter HDD at Toronto (1981‒2010)")
plt.xlabel("Year")
plt.ylabel("HDD")
```
```{python, include = FALSE}
plt.savefig('l1f3.png')
plt.clf()
```

![](l1f3.png)

Let's see if we can detect any trends in our baseline values. For convenience, I am going to create a function that will spit out the relevant values. You can run these commands directly, but this might save you time when you get to the exercises (Hint!).

```{python}
def test_trends(years, values):
    slope, intercept, r_value, p_value, std_err = stats.linregress(years, values)
    print("The regression coefficients are", np.round(slope, 3), "for the slope and",
          np.round(intercept, 1), "for the intercept\n")

    t_crit = stats.t.ppf(0.975, len(years) - 1)
    confidence_interval = t_crit * std_err
    print("The true value of the slope is then", np.round(slope, 3), "+/-",
          np.round(confidence_interval, 3),"\n")

    pearsons_corrcoef, p_corr = stats.pearsonr(years,  values)
    levels = [0.001, 0.01, 0.05, 0.1]
    lvl = [i for i in levels if p_corr < i]
    print("The correlation is", np.round(pearsons_corrcoef, 3), "with a p-value of",
          np.round(p_corr, 5), "(not significant)\n" if lvl == [] else
          "(significant at the " + str(min(lvl)) + " level)\n")

    print("The variance in", values.name, "explained by the linear trend is",
          "quantified by the R²: R² =",
          np.round(100 * pearsons_corrcoef**2, 3), '%.\n')
```


```{python}
test_trends(tor_trop.Date, tor_trop.TropNight)
```

```{python}
test_trends(tor_hdd.Year, tor_hdd.HDD)
```

## Exercises (what to submit)

- You have been assigned an exposure unit and period. Write a brief description of the exposure unit, whether there were any missing or suspicious data, and how these data were treated. [2 marks] 
- Include the time series plots with fitted trend lines for your exposure unit and for the relevant temperature variable, e.g. MeanTemp for freezing degree days. [2 marks] 
- Include a table with the summary statistics for your exposure unit and temperature variable. [2 marks] 
- Read both of the "methods" sections above, for obtaining Canadian climate data. You will notice that we have chosen different dates at which to "append" the `Tor2` object to `Tor1` in each example. Repeat your analysis using the other merge date. Have your results changed? Briefly (one sentence is fine) discuss the selection of an appropriate merge date when working with separate data sets. [2 marks] 
- Be sure to clearly label your plots and table and include concise figure and table captions. [2 marks]
