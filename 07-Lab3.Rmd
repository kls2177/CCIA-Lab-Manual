# Lab exercise 3: Navigating across three dimensions of data

_Last update: `r file.info("07-Lab3.Rmd")$mtime`_

```{r setup, include=FALSE}
library(reticulate)
conda <- ifelse(file.exists("/opt/conda/bin/conda"), "/opt/conda/bin/conda", "/data/home-ext/miniconda3/bin/conda")
use_condaenv("man_ccia", conda = conda, required = TRUE)
```

```{python py_setup, include=FALSE}
import matplotlib as mpl
import seaborn as sns
mpl.use("agg") # Needed for the book only
mpl.rc('font', size = 12)
from matplotlib import pyplot as plt
plt.switch_backend('agg') # Might be overkill
import numpy as np
import pandas as pd
pd.set_option('display.max_columns', 10)
import datetime as dt
import re
from netCDF4 import Dataset, date2index, num2date, date2num
```

In this lab, we are going to be exploring three dimensional data contained in NetCDF files. It is strongly recommended that you have a browser tab open with the **netCDF4** library [documentation](http://unidata.github.io/netcdf4-python/) for reference.

We begin, as always, by importing the libraries that we will need.

```{python, eval=FALSE}
import datetime as dt
import numpy as np
import pandas as pd
import re
from matplotlib import pyplot as plt
from netCDF4 import Dataset, date2index, num2date, date2num
```

Now, create a list of the NetCDF files that we will examine. We have created some miniature NetCDF files for use in this book, but you should substitute the directories and filenames below with paths leading to the NetCDF files that you downloaded in Lab 2.  

```{python}
past = "data/tas_Amon_CanESM2_historical_r1i1p1_198001-200512_mini.nc"
future = ["data/tas_Amon_CanESM2_rcp26_r1i1p1_200601-204012_mini.nc", "data/tas_Amon_CanESM2_rcp85_r1i1p1_200601-204012_mini.nc"]
```

## The past

We will begin by looking at our "historical" NetCDF file. We can load this file into memory using the `Dataset()` function that we imported from **netCDF4**.

```{python}
nc = Dataset(past)
```

First, we recommend that you get familiar with your data set. For instance, what are the dimensions of your data? _Remember, yours should look much different than the output, below_.

```{python}
print(nc.dimensions)
```

Notice that we have three-dimensional data, stored along X (longitude), Y (latitude), and T (time). Some NetCDF files also have a Z dimension (e.g. height). There is no hard limit to the number of dimensions a NetCDF file can have, though [it is suggested](https://www.unidata.ucar.edu/software/netcdf/docs/group__dimensions.html) not to exceed 1024. Rest assured, it is unlikely that you will encounter a 1024-D file in the wild.

You can get a list of variables stored in your file using `nc.variables.get()`. The output is very long, so we will omit it here. You can query a specific variable as well, e.g. `time`.

```{python}
print(nc.variables.get('time'))
```

The above tells us interesting attributes about our variable, such as units, current shape, and whether it is an unlimited variable or not. 

We can store the variable in a Python object, which will help us to get the its values. 

```{python}
nc_time = nc.variables.get("time")
print(nc_time[0:10])
```

### Get subset indices {#getindices}

The time variable isn't very easy for us to read, as it is stored as "Days since January 1, 1850". Luckily, there are a few helper functions to help us get our period of interest: 1981 to 2010. 

```{python}
time_start = date2index(dt.datetime(1981, 1, 1), nc_time, select="nearest")
time_end = date2index(dt.datetime(2010, 12, 31), nc_time, select="nearest")
```

The observant among you may be asking how we can get data up to 2010 in a file that only contains data to 2005. We can't! We have selected the index that contains data _nearest_ to our desired end date. If we check the true date, we will see that we only have data to 2005. 

```{python}
print(num2date(nc_time[time_end], units=nc_time.units, calendar=nc_time.calendar))
```

Most of the CMIP5 climate models force data with the RCPs as of January 2006. This means that we will need to be conscious of the fact that the last five years of our baseline will be "forced". We will deal with this in upcoming labs.

We now know the slice of T that we need. How about X and Y? We will select a single grid cell, based on our station target. First, I will define that target. I have chosen to use a Python dictionary, because I have a tendency to get my "lats" and my "lons" confused. You can use a regular list if you prefer. 

```{python}
# Toronto station coords
target = {"lat": 43.67, "lon": -79.40}
```

We are used to using negatives for western longitudes (i.e. 180°W to 180°E), but the CMIP5 models use longitudes from 0°E to 360°E instead. We'll need to convert our longitude.

```{python}
if target['lon'] < 0:
    target['lon'] = 180 - target['lon']

print(target['lon'])
```

Now we can get the data for our latitude and longitude variables. Notice the syntax below. We query the variable using `nc.variables.get()`, then we choose the full range of the data using `[:]`, which returns a masked array. We pull the values out of that array with `.data`. 

```{python}
nc_lat = nc.variables.get("lat")[:].data
nc_lon = nc.variables.get("lon")[:].data
```

Now we can find the latitude and longitude cell that is closest to our target. 

```{python}
lat_cell = np.argmin(np.abs(nc_lat - target['lat']))
lon_cell = np.argmin(np.abs(nc_lon - target['lon']))
```

```{block, type='rmdtip'}
The minimal NetCDF files that we've created for this book do not include "bounds" variables, however, these can be useful. It is not always clear what the "latitude" and "longitude" of a cell refers to: is the centre point? Is it the top-left corner of the grid box? This is where the bounds variables are really useful. The latitude bounds provide the latitude of the top and bottom of each grid box. The longitude bounds provide the right and left sides of the box. Using the bounds can be a safer way to identify the grid box that we are interested in. Depending on the model you have selected, look for a variable called `lat_bnds` or `lat_bounds`.

`nc_lat_bnds = nc.variables.get("lat_bnds")[:].data`<br>
`lat_cell = np.where((nc_lat_bnds[:,0] < target['lat']) & (nc_lat_bnds[:,1] > target['lat']))[0][0]`<br>
`nc_lon_bnds = nc.variables.get("lon_bnds")[:].data`<br>
`lon_cell = np.where((nc_lon_bnds[:,0] < target['lon']) & (nc_lon_bnds[:,1] > target['lon']))[0][0]`
```

### Getting the data

Now that we know the indices of X, Y, and T that we are interested in, we can pull these out of the NetCDF file. 

```{python}
dat = nc.variables['tas'][time_start:(time_end + 1), lat_cell, lon_cell]
print(dat[0:10])
```

You may have noticed temperatures in the 200s. Our data is in Kelvin. To convert to Celcius, we can subtract 273.15 from every value. For convenience, I will also pull the values out of the masked array using `.data`.

```{python}
dat = np.subtract(dat.data, 273.15)
```

Let's make sure that we got the right amount of time and data.

```{python}
print(time_end - time_start + 1) # 25 yr * 12 mo = 300
```

```{python}
print(len(dat))
```

We can convert our time deltas into dates as follows:

```{python}
dates = num2date(nc_time[time_start:time_end + 1], units=nc_time.units, calendar=nc_time.calendar)
```

Now create a Pandas data frame.

```{python}
df = pd.DataFrame({'Date': dates, 'MeanTemp': dat, 'Experiment': 'historical'})
print(df.head())
```

The calendar used in CanESM2 is a "365 day" calendar, which ignores leap years. The `num2date` function above, converts the times into a special `cftime.DatetimeNoLeap` object. Unfortunately, this date format does not react intuitively in plots. Given that we are using monthly data, it doesn't matter to us whether there were 28 or 29 days in February of any given year. As such, let's convert the `cftime.DatetimeNoLeap` object to a regular Python `datetime.date`. 

```{python}
df['Date'] = [dt.date(i.year, i.month, i.day) for i in df.Date]
```

Let's see what our data frame looks like.

```{python}
print(df.head())
```

```{python}
print(df.tail())
```

We can plot the results of our analysis up to this point.

```{python}
plt.plot(df.Date, df.MeanTemp)
plt.title(r'CanESM2 Monthly $T_{mean}$ at Toronto (1981‒2005)')
plt.xlabel("Year")
plt.ylabel("Mean Temperature (°C)")
```
```{python, include = FALSE}
plt.savefig('l2f1.png')
plt.clf()
```

![](l2f1.png)

It is always a good idea to close our NetCDF file when we are done with it. 
```{python}
nc.close()
```

## The future

I will condense the above process into a quick `for` loop across the two NetCDF files that contain projections. In your case, you may have three or four such files. Also note that I will only extract data to 2040, but you should use 2100 as your upper date. A reminder of the contents of `future`:

```{python}
print(future)
```

This loop will process the data from each file in the same way that we processed the historical data, and will create one data frame containing all of our data. 

```{python}
for file in future:
    nc = Dataset(file)
    nc_time = nc.variables.get("time")
    time_start = date2index(dt.datetime(2006, 1, 1), nc_time, select="nearest")
    time_end = date2index(dt.datetime(2040, 12, 31), nc_time, select="nearest")
    lat_cell = np.argmin(np.abs(nc_lat - target['lat']))
    lon_cell = np.argmin(np.abs(nc_lon - target['lon']))
    dat = nc.variables['tas'][time_start:(time_end + 1), lat_cell, lon_cell]
    dat = np.subtract(dat.data, 273.15)
    dates = num2date(nc_time[time_start:time_end + 1], units=nc_time.units, calendar=nc_time.calendar)
    df2 = pd.DataFrame({'Date': dates, 'MeanTemp': dat, 'Experiment': re.findall(r'rcp[24568]{2}', file)[0]})
    df2['Date'] = [dt.date(i.year, i.month, i.day) for i in df2.Date]
    df = df.append(df2, sort = True)
    nc.close()
```

The table that we have created above, is a "narrow" data frame. A narrow data frame is what is referred to as a "tidy" data frame in the R universe. Tidy data is characterized by a few simple principles: each column is a variable, each row is an observation, each table contains a unique type of observational unit.

A narrow layout table is very useful for many operations such as grouping and subsetting, but can be a challenge to plot. For instance: 

```{python}
df.plot()
```
```{python, include = FALSE}
plt.savefig('l2f2.png')
plt.clf()
```

![](l2f2.png)

That doesn't look right. We can manually fix the above by filtering for each projection and adding the line manually. 

```{python}
plt.plot(df.Date[df.Experiment == "rcp26"], df.MeanTemp[df.Experiment == "rcp26"], 'y-', alpha=0.7, label = 'RCP 2.6')
plt.plot(df.Date[df.Experiment == "rcp85"], df.MeanTemp[df.Experiment == "rcp85"], 'r-', alpha=0.7, label = 'RCP 8.5')
plt.legend(loc='lower right')
plt.title(r'CanESM2 Monthly $T_{mean}$ at Toronto (2006‒2040)')
plt.xlabel("Year")
plt.ylabel("Mean Temperature (°C)")
```
```{python, include = FALSE}
plt.savefig('l2f3.png')
plt.clf()
```

![](l2f3.png)

What if we want to include the "historical" data? The **seaborn** library can also make this process a little easier. Run the following if you have installed **seaborn**. 

```{python}
import seaborn as sns
fg = sns.FacetGrid(hue="Experiment", data=df, aspect=1.61)
fg.map(plt.plot, "Date", "MeanTemp").add_legend()
plt.title(r'CanESM2 Monthly $T_{mean}$ at Toronto (1981‒2040)')
plt.ylabel("Mean Temperature (°C)")
```
```{python, include = FALSE}
plt.savefig('l2f4.png')
plt.clf()
```

![](l2f4.png)

If we want to create a table for publication, usually the wide layout is more print-friendly. You can create a wide layout table using `df.pivot` like this:

```{python}
df_wide = df.pivot(index='Date', columns='Experiment')['MeanTemp']
print(df_wide.head())
```

```{python}
print(df_wide.tail())
```

This is also easier to plot with the `plot` method built into **pandas**. 

```{python}
df_wide.plot(title = r'CanESM2 Monthly $T_{mean}$ at Toronto (1981‒2040)')
plt.ylabel("Mean Temperature (°C)")
```
```{python, include = FALSE}
plt.savefig('l2f5.png')
plt.clf()
```

![](l2f5.png)

## Change Factors

In out next lab, we are going to use change factors to describe changes in our baseline climate. The change factor method (sometimes called $\Delta T$) calculates the projected temperature by adding the observed, baseline temperature time series to the change in the temperature projected by a CGM integration (typically using monthly GCM data). This process removes the bias in the GCM baseline climatology, and represents a simplified form of downscaling. Change factors are described by the following equation:  

<center>$T_{\mathrm{FUTURE}} = T_{\mathrm{OBSERVED}} + \Delta T_{\mathrm{GCM}}$</center>

The first step for a change factor analysis is to calculate the change projected by a GCM over its baseline period. To do so, we will use 30-year "tridecades", including our baseline (1981-2010), and three common climate change tridecades: the 2020s (2011--2040), the 2050s (2041--2070), and the 2080s (2071--2100). 

Let's begin by filtering our "narrow" data frame to our areas of interest. I will only provide examples for the baseline and the 2020s, but you should complete this work for the 2050s and 2080s as well. 

```{python}
df_base = df[(df.Date >= dt.date(1981, 1, 1)) & (df.Date <= dt.date(2010, 12, 31))]
df_2020s = df[(df.Date >= dt.date(2011, 1, 1)) & (df.Date <= dt.date(2040, 12, 31))]
```

Recall from \@ref(getindices) that our baseline is going to be partially forced by each climate change scenario. As such, we will need to calculate a baseline value for each RCP. 

```{python}
bsln26 = df_base[df_base.Experiment.isin(["historical", "rcp26"])].MeanTemp.mean()
bsln85 = df_base[df_base.Experiment.isin(["historical", "rcp85"])].MeanTemp.mean()
```

Now we can calculate the change factor (or anomaly) for each of our tridecadal periods.

```{python}
cf2020s_rcp26 = df_2020s[df_2020s.Experiment == "rcp26"].MeanTemp.mean() - bsln26
cf2020s_rcp85 = df_2020s[df_2020s.Experiment == "rcp85"].MeanTemp.mean() - bsln85
```

We can summarize these data in a table like so:

```{python}
print(pd.DataFrame({"Model": np.repeat("CanESM2", 4),
                    "Ensemble": np.repeat("r1i1p1", 4),
                    "Scenario": ["RCP2.6", "RCP4.5", "RCP6.0", "RCP8.5"],
                    "Baseline (°C)": [bsln26, '', '', bsln85],
                    "2020s": [cf2020s_rcp26, '', '', cf2020s_rcp85],
                    "2050s": np.repeat([''], 4),
                    "2080s": np.repeat([''], 4)}))
```

```{block, type='rmdassignment'}
- In the table above, the 2020s change factor for the RCP 2.6 scenario was _higher_ than that of the RCP 8.5 scenario. How can that be? Explain the "narrative" quality of the RCP scenarios. [2 marks]
- Complete the table, above, for the model that you have used throughout this lab. [2 marks]
- Using [_Conjuntool_](https://shiny.conr.ca/conjuntool/), create a _.csv_ file for all of the models that are available for ensemble r1i1p1. [2 marks]
  - Click on the "GCM Anomalies" tab.
  - Under "1: Def. params", enter the coordinates for Toronto in the "Location" box.
  - Under "2: Add. filters", ensure that only r1i1p1 is selected
  - Under "4: Go!", click the "Process!" buttom and wait for the results.
  - Under "5. Add. Opts.", select the "Calculate Anomalies" box. 
  - Click the "Download Table" option below the table.
- Compare the "baselines" of the models from the _Conjuntool_ output. How can you explain the similarity or differences? 
[2 marks]
- Provide a brief summary of the _Conjuntool_ output. Which model projects the largest change in temperatures? Which 
projects the smallest? What is the overall average baseline and change factor for each period across all models? [2 marks]
```
