# Strategies for selecting GCM models {#modelselection}

_Last update: `r file.info("08-model_selection.Rmd")$mtime`_

Note, for more detailed reading on some of the topics covered in this chapter, see @anderson_2018_efficient.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(readr)
library(dplyr)
library(plotly)
library(stringr)
```

One of the challenges facing students and practitioners of climate change impact assessment is deciding which models to use for the assessment. Have a look at Figure \@ref(fig:ts-plot). The black line is the actual observed Toronto annual average $T_{\textrm{mean}}$ (produced from the data that we downloaded from Environment Canada). Each of the coloured solid lines represents the historical scenario of some model from the r1i1p1 ensemble. The dashed lines, after around 2005, represent the RCP scenario-forced projections for future climate in the same grid cell. 

```{r ts-plot, message=FALSE, warning=FALSE, fig.cap="Observed and GCM-modelled annual average mean temperature at Toronto (1981\u20122100). The black line traces observed data from the Toronto climate station. Each coloured line represents the projection from one of 39 CMIP5 GCM models. Solid lines indicate historical projections; broken lines are projections forced with the four IPCC RCP scenarios. Mouse over the figure for a detailed tooltip."}

tor <- read_csv("tor.csv") %>% select(-X1)
source("R/aggregate_with_missing.R")
tor_annual <- aggregate_with_missing(tor)
rm(tor, aggregate_with_missing)

tor_anoms <- read_csv("data/cjt_tor_anoms.csv")
models <- tor_anoms %>%
  filter(!(Model %in% c("HadCM3", "MIROC4h"))) %>%
  mutate(Model = gsub("([0-9])-([0-9])", paste0("\\1", ".", "\\2"),
                      gsub("([0-9])-([0-9])", paste0("\\1", ".", "\\2"), Model))) %>%
  select(Model) %>% unlist %>% unique

gcm  <- readRDS("data/cjt_tor_tas_fullts.rds") %>%
  filter(Model %in% models & Year >= 1981 & Year <= 2100) %>%
  group_by(Var, Model, Scenario, Ensemble, Year) %>%
  dplyr::select(-Time, -Month) %>%
  summarize(Value = mean(Value))

gcm <- gcm %>% ungroup %>% mutate(Scenario = case_when(
  str_detect(Scenario, "historical/") & Year <= 2005 ~ "historical",
  str_detect(Scenario, "historical/") & Year >= 2006 ~ sub("historical/", "", Scenario),
  TRUE ~ Scenario)) %>%
  filter(!(str_detect(Model, "HadGEM") & Year == 2005 & Scenario != "historical")) %>%
  distinct()

to_plot <- tor_annual %>% mutate(Scenario = "historical", Value = MeanTemp)

p_gcm <- plot_ly(data = gcm, x = ~Year, y = ~Value, color = ~Model, linetype = ~Scenario,
                 hoverinfo = "text",
                 hovertext = sprintf("%s: %s (%s): %2.1f°C", gcm$Model, gcm$Year,
                                     gcm$Scenario, gcm$Value)) %>%
  add_lines() %>% add_lines(data = to_plot, color = I("black"), hoverinfo = "text",
                 hovertext = sprintf("Observed: %s, (historical): %2.1f°C", to_plot$Year,
                                     to_plot$Value)) %>%
  layout(margin = list(b = 60),
         yaxis = list(title = "<i>T</i><sub>mean</sub> Temperature (°C)"), 
         showlegend = FALSE)

p_gcm
```

So, why do we see so much variation? There are a couple of factors at play here. Perhaps most importantly, is the fact that we are comparing apples to oranges, or, more specifically, an apple to an orchard! Each of these model projections is for a _grid cell_ that contains the specific coordinates of the Toronto weather station. Toronto has a lot of local climate considerations like a strong Urban Heat Island and an important Lake Breeze, which makes it stand out in the context of the larger region. These processes are not well represented in the climate models, some of which don't even consider the Great Lakes!

Another consideration is the fact that each model will display its own bias. The modelling groups that have created these models only focus on "local areas" in the broadest sense of the word (e.g., the Arctic), unless there is a very specific, large bias in a particular region that needs addressing. Biases typically arise due to biases in the representation of physical processes (usually in parameterizations) or due to biases generated by inadequate horizontal or vertical resolution. Just like the climate of a model, the biases are emergent and are actually quite hard to remedy on a local scale. These biases mean that a model that performs very well over the deserts of Australia may not perform as well over the Arctic. 

How, then, do we choose which models to use to inform our climate change impact assessment? There are a number of strategies that we can employ for the task, including using models at the extreme top and bottom of the projected range, using the average of all models in a multi-model ensemble, or selecting "validated" models based on the models' ability to reproduce the observed baseline climate. For further detail on each of these methods, read Section 1.1 of @anderson_2018_efficient (find the PDF at the link in the reference, below), then come back here to begin the lab.
